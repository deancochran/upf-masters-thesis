{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction with RHGNN using the LastFM1b Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objectives of this notebook\n",
    "\n",
    "In this notebook, we will be combing the knowledge presented in a modern Heterogeneous Graph Neural Network topic (RHGNNs) with a customized collection of data. \n",
    "This will be achieved in hopes to show the applications of RHGNNs for downstream tasks like node classification and link prediction. \n",
    "\n",
    "More specifically, this notebook will utilize the dataset provided by LastFM, called LastFM1b. With this collection of listening events \n",
    "we will create a heterogeneous graph that our machine learning model with utilize to create high-quality embeddings. \n",
    "Once this is finished we will utilize the embeddings to find patterns in user listening behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breif Intro to RHGNN\n",
    "Representational Lerning for Hetergeneous Graph Neural Networks is a trending topic in machine Learning. \n",
    "Most heterogenous methods are utilized for propagation of singular node representations, whereas in RHGNN's method, \n",
    "the relational information that exists between nodes is utilized for imporving the noderepresentations. \n",
    "Each convolutional component of this model is able to learn node represenations of a singlar relational type. \n",
    "After this is achieved, a \"cross relational\" message passing module is able to improve the node represenations by \n",
    "factoring in the characteristics of the nodes relational connections. These **relationally aware** representations are then \n",
    "passed through a heterogeneous gnn layer stack to allow the model to capture the \"relational semantics.\" This makes \n",
    "RHGNN can encapsulate the characteristics of the relational connections between nodes on a heterogeneous graph.\n",
    "\n",
    "For more info, see the paper \"Representational Learning for Heterogeneous Graph Neural Networks\" here: https://arxiv.org/abs/2105.11122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breif Intro to LFM1b\n",
    "LFM1b dataset consists of more than one billion listening events, intended to be used for various music retrieval and recommendation tasks. \n",
    "A paper describing the dataset was accepted to the ACM International Conference on Multimedia Retrieval (ICMR) 2016 and is available for download. \n",
    "\n",
    "For more info, see the paper \"The LFM-1b Dataset for Music Retrieval and Recommendation\" or the main download page here: http://www.cp.jku.at/datasets/LFM-1b/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methodology of the Notebook\n",
    "\n",
    "Before the notebook starts it is worth mentioning the fundamentals of a library call Deep Graph Library (DGL). DGL is one of the many competing deep learning with graph networks python libaries. \n",
    "Other notebale libraries include, PyTorch Geometric, Spektral, and many others collections. \n",
    "\n",
    "Within this particular notebook we will be utilizing DGL's graph database frameworks to compute complex measurements on large graphs. \n",
    "As with most libraries, I recommend reading through the user guide in the documentation, as well as maybe another library like pytorch geometric to see the similarities and differeneces.\n",
    "\n",
    "With all of this being said, and for the interested learner, I'll refer everyone to the Standford CSS224W course on Graphs for Machine Learning here: https://web.stanford.edu/class/cs224w/. \n",
    "Many more resources are given as you work your way through the course, which I've found to help my understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Here's a note on the requirements needed to operate this notebook...\n",
    "\n",
    "You will want a GPU, it might take to long otherwise. \n",
    "Additionally the following requirements:\n",
    "\n",
    "* PyTorch 1.7.1\n",
    "* DGL 0.5.3\n",
    "* PyTorch Geometric 1.6.3\n",
    "* OGB 1.3.1\n",
    "* tqdm\n",
    "* numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: right;\n",
       "    vertical-align: middle;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import torch as th \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "from utils.utils import set_random_seed, get_edge_data_loader, get_predict_edge_index, convert_to_gpu, get_n_params, get_optimizer_and_lr_scheduler, evaluate_link_prediction\n",
    "from dgl.data.utils import load_graphs\n",
    "from utils.LinkScorePredictor import LinkScorePredictor\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from model.R_HGNN import R_HGNN\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: right;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARGS\n",
    "SEED=0\n",
    "SAMPLE_EDGE_RATE=0.01\n",
    "# SAMPLED_EDGE_TYPE='listened_to_track'\n",
    "# SAMPLED_EDGE_TYPE='listened_to_album'\n",
    "SAMPLED_EDGE_TYPE='listened_to_artist'\n",
    "NODE_NEIGHTBORS_MIN_NUM = 10\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 1024\n",
    "NEGATIVE_SAMPLE_EDGE_NUM= 5\n",
    "SHUFFLE = True\n",
    "DROP_LAST = False\n",
    "NUM_WORKERS = 4\n",
    "HIDDEN_DIM = 32\n",
    "RELATIONAL_INPUT_DIM = 20\n",
    "RELATIONAL_HIDDEN_DIM = 8\n",
    "N_HEADS = 8\n",
    "DROPOUT = 0.3\n",
    "RESIDUAL = True\n",
    "NORM = True\n",
    "HID_DIM = 32\n",
    "N_HEADS = 8\n",
    "LINK_SCORE_PREDICTOR = LinkScorePredictor(HID_DIM * N_HEADS)\n",
    "OPTIMIZER_NAME = 'adam'\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0\n",
    "EPOCHS = 200\n",
    "DEVICE='cuda'\n",
    "MODEL_NAME='R_HGNN'+'_'+SAMPLED_EDGE_TYPE\n",
    "SAVE_MODEL_FOLDER = f\"../save_model/'lfm1b'/{MODEL_NAME}\"\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in heterogenous data \n",
    "\n",
    "As a mindful note, graphs are everywhere. In many cases with large databases,\n",
    "there exists a graphical model that can represent the complex interconnections data in a single visualize. \n",
    "If this doesn't make to much since I recommend reading through some of the references above or taking a look at hese visuals\n",
    "\n",
    "Specifcally a heterogenous graph is a colleciton of sets of nodes/vertices (V), edges/links (E), node types (A), and edge types (R) such that\n",
    "<center>\n",
    "<img\n",
    "  src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;G=(V,E,A,R),&space;where&space;\\begin{vmatrix}A&space;\\\\\\end{vmatrix}&space;&plus;&space;\\begin{vmatrix}&space;R&space;\\\\\\end{vmatrix}&space;>2\"\n",
    "  />\n",
    "</center>\n",
    "<br>\n",
    "<center>\n",
    "<img\n",
    "  src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;e=(\\phi\\left&space;(u&space;&space;\\right&space;),\\psi\\left&space;(e&space;&space;\\right&space;),\\phi\\left&space;(v&space;&space;\\right&space;)),&space;where&space;\\left\\{u,&space;v&space;\\epsilon&space;V&space;\\right\\},&space;\\left\\{e&space;\\epsilon&space;E&space;\\right\\},&space;\\left\\{\\phi\\left&space;(u&space;&space;\\right&space;),\\phi\\left&space;(v&space;&space;\\right&space;)&space;\\epsilon&space;A&space;\\right\\},&space;\\left\\{\\psi\\left&space;(e&space;&space;\\right&space;)&space;\\epsilon&space;R&space;\\right\\}\"\n",
    "/>\n",
    "</center>\n",
    "\n",
    "\n",
    "In many cases, this can be through of as a knowledge graph as well. These concepts tend to be blurred by the task at hand. **Here is an example of a heterogenous graph from the OBG baseline dataset found on PyTorch's documentation**\n",
    "\n",
    "<center>\n",
    "<img\n",
    "  src=\"https://pytorch-geometric.readthedocs.io/en/latest/_images/hg_example.svg\"\n",
    "  />\n",
    "</center>\n",
    "\n",
    "For our purposes, we'll being using a custom made dataset :) It iterates through all the one billion listening events and uses the dgl heterogrpah object \n",
    "to create a massive heterogenous model for our machine learning model to work with.\n",
    "\n",
    "Do mind this warning: This is note the full dataset being used in this notebook. Rather in this notebook we'll just pull the first 10 million\n",
    "listening events and to collect the unique user, artist, album and track ids that exist in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'album': 9702, 'artist': 4775, 'track': 28185, 'user': 17},\n",
       "      num_edges={('album', 'listened_to_album-rev', 'user'): 10115, ('album', 'produced_by', 'artist'): 9702, ('artist', 'listened_to_artist-rev', 'user'): 5604, ('artist', 'preformed_by-rev', 'track'): 28185, ('artist', 'produced_by-rev', 'album'): 9702, ('track', 'listened_to_track-rev', 'user'): 29584, ('track', 'preformed_by', 'artist'): 28185, ('user', 'listened_to_album', 'album'): 10115, ('user', 'listened_to_artist', 'artist'): 5604, ('user', 'listened_to_track', 'track'): 29584},\n",
       "      metagraph=[('album', 'user', 'listened_to_album-rev'), ('album', 'artist', 'produced_by'), ('user', 'album', 'listened_to_album'), ('user', 'artist', 'listened_to_artist'), ('user', 'track', 'listened_to_track'), ('artist', 'user', 'listened_to_artist-rev'), ('artist', 'track', 'preformed_by-rev'), ('artist', 'album', 'produced_by-rev'), ('track', 'user', 'listened_to_track-rev'), ('track', 'artist', 'preformed_by')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using DGl's load_graphs function to load pre-computed and processed files\n",
    "glist,_=load_graphs('lastfm1b_subset.bin') # <- this file represents a subset of the full dataset\n",
    "hg=glist[0] # hg=='heterogeneous graph' ;) from the list of graphs in the processed file (hint: theres only one) pick our heterogenous subset graph\n",
    "hg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how DGL represents their HeteroData Object, for more info see there documentation...\n",
    "\n",
    "You might see that in our meta-graph variable, there are also reverse edges labeled conviently with a '-rev' ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'listened_to_album-rev': 'listened_to_album',\n",
       " 'produced_by': 'produced_by-rev',\n",
       " 'listened_to_artist-rev': 'listened_to_artist',\n",
       " 'preformed_by-rev': 'preformed_by',\n",
       " 'produced_by-rev': 'produced_by',\n",
       " 'listened_to_track-rev': 'listened_to_track',\n",
       " 'preformed_by': 'preformed_by-rev',\n",
       " 'listened_to_album': 'listened_to_album-rev',\n",
       " 'listened_to_artist': 'listened_to_artist-rev',\n",
       " 'listened_to_track': 'listened_to_track-rev'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dictionary of every edge and it's reverse edge\n",
    "reverse_etypes = dict()\n",
    "for stype, etype, dtype in hg.canonical_etypes: # for every edge type structured as (phi(u), psi(e), phi(v))\n",
    "    for srctype, reltype, dsttype in hg.canonical_etypes:\n",
    "        if srctype == dtype and dsttype == stype and reltype != etype:\n",
    "            reverse_etypes[etype] = reltype\n",
    "            break\n",
    "reverse_etypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our data, we are tasked with generating representations for each these nodes in a latent space. Once this is achieved, we can utilize \n",
    "different algorithms to generate predictions. However, throughout the academic research of GNNs, the authors of the published RHGNN paper have brought up a very important flaw that most \n",
    "traditional GNN models do not address. Specifically, many of these algorithms do not utilize the relational information that exists between \n",
    "nodes of different relatinoal edges for generating node representations.\n",
    "\n",
    "As we continue through this notebook we will continue this dicussion of why it is important to utilize the relational information that exists in a \n",
    "graph for creating high quality node embedding representations. For now however, we need to understand what we will need for our task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction on Heterogeneous Graphs\n",
    "\n",
    "Simply put, this is the task for prediting the probability of a edge exsisting between two nodes in a graph. Mathematically, we can present it as \n",
    "The likelihood of connectivity between two nodes u and v such that\n",
    "\n",
    "<center><img\n",
    "src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;y_{u,v}=\\phi\\left&space;(&space;h_{u}^L&space;,&space;h_{v}^L\\right&space;)\"\n",
    "/></center>\n",
    "\n",
    "where we have a function \n",
    "<img\n",
    "src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;\\phi&space;\"\n",
    "/>\n",
    "to predict the likelihood of an edge existing between the embedding representations that our GNN model is capable of computing\n",
    "<img\n",
    "src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;h_{u}^L,&space;h_{v}^L\"\n",
    "/>\n",
    "\n",
    "If you care to theorize, you can notice that we haven't added the edge type into this equation. This is due to the fact that for hetergeneous models, \n",
    "we will have separate functions for each edge type that exists in the graph. (Or a least just the edges that we want to predict for a particular task).\n",
    " With this intution in mind, what does this mean for our heterogenous graph? This means that with our data we will be able to preform link prediciton for any of the edge types that exist! \n",
    "\n",
    "In this notebook we will just be working on predicting the likelihood of a user listening to a track, but the notebook can easily be extended to work with different edges\n",
    "\n",
    "In this notebook the SAMPLED_EDGE_TYPE, references which link we want our model to learn to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train edge num: 168, valid edge num: 56, test edge num: 56\n"
     ]
    }
   ],
   "source": [
    "train_edge_idx, valid_edge_idx, test_edge_idx = get_predict_edge_index(\n",
    "    hg,\n",
    "    sample_edge_rate=SAMPLE_EDGE_RATE,\n",
    "    sampled_edge_type=SAMPLED_EDGE_TYPE,\n",
    "    seed=SEED)\n",
    "    \n",
    "print(f'train edge num: {len(train_edge_idx)}, valid edge num: {len(valid_edge_idx)}, test edge num: {len(test_edge_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Val, Test Splits for link Prediction\n",
    "\n",
    "Now the reason we stop here to discuss more about link prediction is due to our model needing to learn to compare scores between nodes \n",
    "connected by an edge against the scores between an arbitrary pair of nodes. (Wait what?)\n",
    "\n",
    "Link prediction is a common unsupervised or self-supervised task. Meaning we need to split the our graph and create corresponding labels \n",
    "for our edges to train our model on. For training with Graph Neural Networks there exist two general methods for this (more have been discovered during academic research):\n",
    "\n",
    "\n",
    "Inductive Splits:\n",
    "\n",
    "    Where the training, validation, and test sets are different graphs. When using this approach a successful model should be able generalize unseen graphs for node, edge, graph level tasks\n",
    "\n",
    "Transductive Splits: \n",
    "\n",
    "    Where the training, validation, and test sets exist all on the same graph. This might not be so intuitive to think about, but the original full graph has all the splits, but the labels for the edges are different. This is specifically only applicable to node or edge level tasks\n",
    "\n",
    "\n",
    "For example, given an edge connecoing node u and v, the model will train to score between \n",
    "node u and v to be higher than a score between node ùë¢ and another node v' from an arbitrary noise distribution v'~P(v). \n",
    "This is known as a funamental concept of negative sampling. Because a score prediction model operates on graphs, \n",
    "we need to express the negative samples as graphs. The graph will contain all negative node pairs as edges.\n",
    "\n",
    "So what does this mean for our hetergeous graph, and the inputs into our model of choice? Well it means we first need to utilize our labeled train, validation, and test edges to preform a specific split. \n",
    "For the purposes of this notebook, we will be using a inductive split of the graph.\n",
    "\n",
    "DGL offers the unique ability to split a very large graph for the specific purposes of training a GNN model in a stochastic process. \n",
    "This alows us to generate small training batches that our model can iteratively learn as be evaluate the loss of the model over time.\n",
    "\n",
    "Specifically, well be using the DataLoader object to create iterable objects that can create next batchs of the training, validation, \n",
    "and testing data for the model. To see the DGL implementation see the utils.py file and find the function get_edge_data_loader().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_edge_data_loader(\n",
    "    NODE_NEIGHTBORS_MIN_NUM,\n",
    "    N_LAYERS,\n",
    "    hg,\n",
    "    BATCH_SIZE,\n",
    "    SAMPLED_EDGE_TYPE,\n",
    "    NEGATIVE_SAMPLE_EDGE_NUM,\n",
    "    train_edge_idx=train_edge_idx,\n",
    "    valid_edge_idx=valid_edge_idx,\n",
    "    test_edge_idx=test_edge_idx,\n",
    "    reverse_etypes=reverse_etypes,\n",
    "    shuffle = SHUFFLE, \n",
    "    drop_last = DROP_LAST,\n",
    "    num_workers = NUM_WORKERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection: Relation-aware Heterogeneous Graph Neural Networks (RHGNN)\n",
    "\n",
    "Now that we've prepared our dataset and have compiled the necessary iterators to send train, validation, and testing batches to our model. \n",
    "All that is left is to select the model. Model selection is a very important step in deep learning, it must particularly reflect the goals of the task, \n",
    "as well as utilize the data to it's full capabilities. \n",
    "\n",
    "For our heterogeneous graph we will be using a novel Relation-aware Heterogeneous Graph Neural Network. \n",
    "Published in 2021 by Le Yu, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng Lv, and Hui Xiong, these researchers have implemented a model capable of \n",
    "high quality relationally aware node embeddings that are able to capture characteristics of not only the hetergenous neighbouring nodes, \n",
    "but the relation ships that exist between them. To be breif their research proposes 3 contributions as well as outlines the 4 necessary steps \n",
    "inorder for their models final node representations to be computed.\n",
    "\n",
    "Contributions of the paper:\n",
    "1. A methogolgy to compute relational aware node embeddings\n",
    "2. A methogolgy to compute relational edge embeddings\n",
    "3. A methogolgy to compute a embeddings through a fusing module that utilizes the information of both the pior contributions \n",
    "\n",
    "Computation steps that are used to compute the final node represenations\n",
    "1. Multiple convolutional layers that are able to learn the specific node represenations independently of the relational connections they have\n",
    "2. A Cross Relational Learning module to determine the importance of the edges between the nodes depending on the type of the relationship\n",
    "3. A GNN containing the neccessary deep learning methodologies to utilize the piror computed representations to update the graph\n",
    "4. A fusing aggregate module of relationally aware node representations that results in a singluar compact node representation to facilitate downstream prediction tasks\n",
    "\n",
    "Here is the depiction the authors made for the published paper, it represents the full alogirhtm with the major components mentioned above\n",
    "\n",
    "\n",
    "<center>\n",
    "<img \n",
    "src=\"https://d3i71xaburhd42.cloudfront.net/8a30c43eec88d087d2029c8de1f3a7961b753340/4-Figure2-1.png\"\n",
    "/>\n",
    "</center>\n",
    "\n",
    "\n",
    "For our project, all we have to do is understand how the model works, the implementation of the physical model is actaully importable! \n",
    "If you havent'y noticed already, the directory you are in is actually a modified version of the published repository found here: https://github.com/yule-BUAA/R-HGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics of the RHGNN\n",
    "\n",
    "I'll skip this for now, but don't think you won't see this section again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #Params: 12640208.\n",
      "Sequential(\n",
      "  (0): R_HGNN(\n",
      "    (relation_embedding): ParameterDict(\n",
      "        (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "        (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x1 (GPU 0)]\n",
      "    )\n",
      "    (projection_layer): ModuleDict(\n",
      "      (album): Linear(in_features=9702, out_features=256, bias=True)\n",
      "      (artist): Linear(in_features=4775, out_features=256, bias=True)\n",
      "      (track): Linear(in_features=28185, out_features=256, bias=True)\n",
      "      (user): Linear(in_features=17, out_features=256, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): R_HGNN_Layer(\n",
      "        (node_transformation_weight): ParameterDict(\n",
      "            (album): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "            (artist): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "            (track): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "            (user): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "        )\n",
      "        (relation_transformation_weight): ParameterDict(\n",
      "            (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "            (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 20x512 (GPU 0)]\n",
      "        )\n",
      "        (relation_propagation_layer): ModuleDict(\n",
      "          (listened_to_album-rev): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (produced_by): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (listened_to_artist-rev): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (preformed_by-rev): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (produced_by-rev): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (listened_to_track-rev): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (preformed_by): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (listened_to_album): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (listened_to_artist): Linear(in_features=20, out_features=64, bias=True)\n",
      "          (listened_to_track): Linear(in_features=20, out_features=64, bias=True)\n",
      "        )\n",
      "        (hetero_conv): HeteroGraphConv(\n",
      "          (mods): ModuleDict(\n",
      "            (listened_to_album-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (produced_by): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_artist-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (preformed_by-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (produced_by-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_track-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (preformed_by): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_album): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_artist): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_track): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (res_fc): ModuleDict(\n",
      "          (album): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (artist): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (track): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (user): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (residual_weight): ParameterDict(\n",
      "            (album): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "            (artist): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "            (track): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "            (user): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "        )\n",
      "        (layer_norm): ModuleDict(\n",
      "          (album): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (artist): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (track): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (user): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (relations_crossing_attention_weight): ParameterDict(\n",
      "            (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "        )\n",
      "        (relations_crossing_layer): RelationCrossing(\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "      (1): R_HGNN_Layer(\n",
      "        (node_transformation_weight): ParameterDict(\n",
      "            (album): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "            (artist): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "            (track): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "            (user): Parameter containing: [torch.cuda.FloatTensor of size 256x256 (GPU 0)]\n",
      "        )\n",
      "        (relation_transformation_weight): ParameterDict(\n",
      "            (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "            (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 64x512 (GPU 0)]\n",
      "        )\n",
      "        (relation_propagation_layer): ModuleDict(\n",
      "          (listened_to_album-rev): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (produced_by): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (listened_to_artist-rev): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (preformed_by-rev): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (produced_by-rev): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (listened_to_track-rev): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (preformed_by): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (listened_to_album): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (listened_to_artist): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (listened_to_track): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (hetero_conv): HeteroGraphConv(\n",
      "          (mods): ModuleDict(\n",
      "            (listened_to_album-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (produced_by): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_artist-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (preformed_by-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (produced_by-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_track-rev): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (preformed_by): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_album): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_artist): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "            (listened_to_track): RelationGraphConv(\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "              (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "              (relu): ReLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (res_fc): ModuleDict(\n",
      "          (album): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (artist): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (track): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (user): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (residual_weight): ParameterDict(\n",
      "            (album): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "            (artist): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "            (track): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "            (user): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "        )\n",
      "        (layer_norm): ModuleDict(\n",
      "          (album): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (artist): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (track): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (user): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (relations_crossing_attention_weight): ParameterDict(\n",
      "            (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "            (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32 (GPU 0)]\n",
      "        )\n",
      "        (relations_crossing_layer): RelationCrossing(\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (node_transformation_weight): ParameterDict(\n",
      "        (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "        (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x32x32 (GPU 0)]\n",
      "    )\n",
      "    (relation_transformation_weight): ParameterDict(\n",
      "        (listened_to_album): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (listened_to_album-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (listened_to_artist): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (listened_to_artist-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (listened_to_track): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (listened_to_track-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (preformed_by): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (preformed_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (produced_by): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "        (produced_by-rev): Parameter containing: [torch.cuda.FloatTensor of size 8x8x32 (GPU 0)]\n",
      "    )\n",
      "    (relation_fusing): RelationFusing(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      "  (1): LinkScorePredictor(\n",
      "    (projection_layer): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "r_hgnn = R_HGNN(graph=hg,\n",
    "                input_dim_dict={ntype: hg.nodes[ntype].data['feat'].shape[1] for ntype in hg.ntypes},\n",
    "                hidden_dim=HIDDEN_DIM, \n",
    "                relation_input_dim=RELATIONAL_INPUT_DIM,\n",
    "                relation_hidden_dim=RELATIONAL_HIDDEN_DIM,\n",
    "                num_layers=N_LAYERS, \n",
    "                n_heads=N_HEADS, \n",
    "                dropout=DROPOUT,\n",
    "                residual=RESIDUAL, \n",
    "                norm=NORM)\n",
    "\n",
    "model = nn.Sequential(r_hgnn, LINK_SCORE_PREDICTOR)\n",
    "model = convert_to_gpu(model, device='cuda')\n",
    "\n",
    "print(f'Model #Params: {get_n_params(model)}.')\n",
    "print(model)\n",
    "\n",
    "optimizer, scheduler = get_optimizer_and_lr_scheduler(\n",
    "    model, \n",
    "    OPTIMIZER_NAME, \n",
    "    LEARNING_RATE, \n",
    "    WEIGHT_DECAY,\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    epochs=EPOCHS)\n",
    "\n",
    "shutil.rmtree(SAVE_MODEL_FOLDER, ignore_errors=True)\n",
    "os.makedirs(SAVE_MODEL_FOLDER, exist_ok=True)\n",
    "patience = 50\n",
    "early_stopping = EarlyStopping(\n",
    "patience=patience, \n",
    "save_model_folder=SAVE_MODEL_FOLDER,\n",
    "save_model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an evaluation function\n",
    "\n",
    "Once we have our model, we'll need a way to measure the model's ability to predict the probability of an edge existing between two nodes with accuracy.\n",
    "This means we'll first need an algorithm to determine the loss of the model.\n",
    "\n",
    "Quick note on that... what loss function are well even using for Link Prediction\n",
    "\n",
    "There are lots of loss functions that can achieve the behavior above if minimized. A non-exhaustive list include:\n",
    "- Cross-entropy loss\n",
    "- BPR loss\n",
    "- Margin loss\n",
    "\n",
    "By training our model to minimize any of the above mentioned loss functions, we will be able to achieve a model that is able to score nodes that \n",
    "should have an edge between them to have a higher score than two nodes who should not have a connection between them.\n",
    "\n",
    "This definition will only be used twice for every epoch of the training process. \n",
    "It will compute the loss of the validation and tests sets after the training process in that specific epoch is complete.\n",
    "This will let use visually see the change in loss over time for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_func, sampled_edge_type, device, mode):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: model\n",
    "    :param loader: data loader (validate or test)\n",
    "    :param loss_func: loss function\n",
    "    :param sampled_edge_type: str\n",
    "    :param device: device str\n",
    "    :param mode: str, evaluation mode, validate or test\n",
    "    :return:\n",
    "    total_loss, y_trues, y_predicts\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        y_trues = []\n",
    "        y_predicts = []\n",
    "        total_loss = 0.0\n",
    "        loader_tqdm = tqdm(loader, ncols=120)\n",
    "        for batch, (input_nodes, positive_graph, negative_graph, blocks) in enumerate(loader_tqdm):\n",
    "            blocks = [convert_to_gpu(b, device=device) for b in blocks]\n",
    "            positive_graph, negative_graph = convert_to_gpu(positive_graph, negative_graph, device=device)\n",
    "            # target node relation representation in the heterogeneous graph\n",
    "            input_features = {(stype, etype, dtype): blocks[0].srcnodes[dtype].data['feat'] for stype, etype, dtype in\n",
    "                              blocks[0].canonical_etypes}\n",
    "\n",
    "            nodes_representation, _ = model[0](blocks, copy.deepcopy(input_features))\n",
    "\n",
    "            positive_score = model[1](\n",
    "                positive_graph, \n",
    "                nodes_representation, \n",
    "                sampled_edge_type).squeeze(dim=-1)\n",
    "            negative_score = model[1](\n",
    "                negative_graph, \n",
    "                nodes_representation, \n",
    "                sampled_edge_type).squeeze(dim=-1)\n",
    "\n",
    "            y_predict = th.cat([positive_score, negative_score], dim=0)\n",
    "            y_true = th.cat(\n",
    "                [th.ones_like(positive_score), \n",
    "                th.zeros_like(negative_score)], dim=0)\n",
    "\n",
    "            loss = loss_func(y_predict, y_true)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            y_trues.append(y_true.detach().cpu())\n",
    "            y_predicts.append(y_predict.detach().cpu())\n",
    "\n",
    "            loader_tqdm.set_description(f'{mode} for the {batch}-th batch, {mode} loss: {loss.item()}')\n",
    "\n",
    "        total_loss /= (batch + 1)\n",
    "        y_trues = th.cat(y_trues, dim=0)\n",
    "        y_predicts = th.cat(y_predicts, dim=0)\n",
    "\n",
    "    return total_loss, y_trues, y_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "With our model defined and out data loaders defined, we can begin training our model. \n",
    "Our loss for our RHGNN model will be the Binary Cross Entropy loss. After every batch training block, \n",
    "we'll calculate the loss, using the ground truth edges, and the predicted edges. \n",
    "To calculate the error we'll use RMSE and MAE, and print the results of each epoch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.743181049823761: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.78s/it]\n",
      "validate for the 0-th batch, validate loss: 1.3066959381103516: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.06s/it]\n",
      "test for the 0-th batch, test loss: 1.2867895364761353: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, learning rate: 0.000999938933078422, train loss: 0.7432, RMSE 0.5232, MAE 0.5146, \n",
      "validate loss: 1.3067, RMSE 0.6517, MAE 0.5029, \n",
      "test loss: 1.2868, RMSE 0.6496, MAE 0.5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.47300535440444946: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 2.6943113803863525: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s]\n",
      "test for the 0-th batch, test loss: 2.6749513149261475: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, learning rate: 0.0009997557473810372, train loss: 0.4730, RMSE 0.3831, MAE 0.3152, \n",
      "validate loss: 2.6943, RMSE 0.7035, MAE 0.4997, \n",
      "test loss: 2.6750, RMSE 0.7035, MAE 0.4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.5474518537521362: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.72s/it]\n",
      "validate for the 0-th batch, validate loss: 2.268500804901123: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 2.2698564529418945: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, learning rate: 0.0009994504881061753, train loss: 0.5475, RMSE 0.3889, MAE 0.2028, \n",
      "validate loss: 2.2685, RMSE 0.6987, MAE 0.4992, \n",
      "test loss: 2.2699, RMSE 0.6990, MAE 0.4996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.4773107171058655: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.75s/it]\n",
      "validate for the 0-th batch, validate loss: 1.3456947803497314: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 1.356033205986023: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, learning rate: 0.0009990232305719944, train loss: 0.4773, RMSE 0.3779, MAE 0.2131, \n",
      "validate loss: 1.3457, RMSE 0.6546, MAE 0.4962, \n",
      "test loss: 1.3560, RMSE 0.6566, MAE 0.4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.4418509602546692: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.74s/it]\n",
      "validate for the 0-th batch, validate loss: 1.004523754119873: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 1.0144116878509521: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, learning rate: 0.0009984740801978985, train loss: 0.4419, RMSE 0.3701, MAE 0.2978, \n",
      "validate loss: 1.0045, RMSE 0.6015, MAE 0.4881, \n",
      "test loss: 1.0144, RMSE 0.6043, MAE 0.4873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.49335741996765137: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.72s/it]\n",
      "validate for the 0-th batch, validate loss: 1.248511552810669: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "test for the 0-th batch, test loss: 1.264796495437622: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, learning rate: 0.0009978031724785245, train loss: 0.4934, RMSE 0.3974, MAE 0.3626, \n",
      "validate loss: 1.2485, RMSE 0.6425, MAE 0.4859, \n",
      "test loss: 1.2648, RMSE 0.6449, MAE 0.4864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.4028228521347046: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 1.705949068069458: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 1.7254064083099365: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, learning rate: 0.000997010672950314, train loss: 0.4028, RMSE 0.3498, MAE 0.2819, \n",
      "validate loss: 1.7059, RMSE 0.6801, MAE 0.4926, \n",
      "test loss: 1.7254, RMSE 0.6816, MAE 0.4935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.3803703486919403: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.67s/it]\n",
      "validate for the 0-th batch, validate loss: 2.0189449787139893: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 2.034691333770752: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, learning rate: 0.0009960967771506667, train loss: 0.3804, RMSE 0.3458, MAE 0.2259, \n",
      "validate loss: 2.0189, RMSE 0.6916, MAE 0.4939, \n",
      "test loss: 2.0347, RMSE 0.6923, MAE 0.4952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.3599415123462677: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.69s/it]\n",
      "validate for the 0-th batch, validate loss: 1.973303198814392: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 1.9923486709594727: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, learning rate: 0.000995061710569696, train loss: 0.3599, RMSE 0.3415, MAE 0.1974, \n",
      "validate loss: 1.9733, RMSE 0.6881, MAE 0.4911, \n",
      "test loss: 1.9923, RMSE 0.6894, MAE 0.4916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.31521475315093994: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 1.6120647192001343: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 1.6416925191879272: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, learning rate: 0.0009939057285945933, train loss: 0.3152, RMSE 0.3182, MAE 0.1818, \n",
      "validate loss: 1.6121, RMSE 0.6633, MAE 0.4759, \n",
      "test loss: 1.6417, RMSE 0.6674, MAE 0.4825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.2545730769634247: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 1.2401551008224487: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 1.2873742580413818: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, learning rate: 0.000992629116446613, train loss: 0.2546, RMSE 0.2768, MAE 0.1802, \n",
      "validate loss: 1.2402, RMSE 0.6059, MAE 0.4329, \n",
      "test loss: 1.2874, RMSE 0.6190, MAE 0.4503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.22433143854141235: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 1.2387347221374512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 1.2629696130752563: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, learning rate: 0.000991232189110701, train loss: 0.2243, RMSE 0.2449, MAE 0.1716, \n",
      "validate loss: 1.2387, RMSE 0.5856, MAE 0.4179, \n",
      "test loss: 1.2630, RMSE 0.5956, MAE 0.4260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.1943541318178177: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.76s/it]\n",
      "validate for the 0-th batch, validate loss: 1.6736814975738525: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 1.7702062129974365: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, learning rate: 0.0009897152912577741, train loss: 0.1944, RMSE 0.2212, MAE 0.1463, \n",
      "validate loss: 1.6737, RMSE 0.6194, MAE 0.4349, \n",
      "test loss: 1.7702, RMSE 0.6377, MAE 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.17319247126579285: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 2.6936516761779785: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 2.7513511180877686: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, learning rate: 0.00098807879715968, train loss: 0.1732, RMSE 0.2118, MAE 0.1069, \n",
      "validate loss: 2.6937, RMSE 0.6754, MAE 0.4751, \n",
      "test loss: 2.7514, RMSE 0.6798, MAE 0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.15245060622692108: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 3.871906280517578: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n",
      "test for the 0-th batch, test loss: 3.927861213684082: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, learning rate: 0.00098632311059685, train loss: 0.1525, RMSE 0.2091, MAE 0.0748, \n",
      "validate loss: 3.8719, RMSE 0.7001, MAE 0.4988, \n",
      "test loss: 3.9279, RMSE 0.7050, MAE 0.5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.10581542551517487: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 4.779523849487305: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n",
      "test for the 0-th batch, test loss: 4.846290588378906: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, learning rate: 0.0009844486647586723, train loss: 0.1058, RMSE 0.1818, MAE 0.0541, \n",
      "validate loss: 4.7795, RMSE 0.7024, MAE 0.4966, \n",
      "test loss: 4.8463, RMSE 0.7100, MAE 0.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.13753531873226166: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.66s/it]\n",
      "validate for the 0-th batch, validate loss: 5.445672512054443: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 5.461729049682617: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, learning rate: 0.00098245592213661, train loss: 0.1375, RMSE 0.1946, MAE 0.0547, \n",
      "validate loss: 5.4457, RMSE 0.7037, MAE 0.4989, \n",
      "test loss: 5.4617, RMSE 0.7076, MAE 0.5044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.13386227190494537: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.65s/it]\n",
      "validate for the 0-th batch, validate loss: 6.003100872039795: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 5.989911079406738: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, learning rate: 0.0009803453744100868, train loss: 0.1339, RMSE 0.1874, MAE 0.0494, \n",
      "validate loss: 6.0031, RMSE 0.7060, MAE 0.5023, \n",
      "test loss: 5.9899, RMSE 0.7086, MAE 0.5052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.1776999533176422: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.64s/it]\n",
      "validate for the 0-th batch, validate loss: 6.618325710296631: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 6.595553398132324: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, learning rate: 0.0009781175423251734, train loss: 0.1777, RMSE 0.2015, MAE 0.0533, \n",
      "validate loss: 6.6183, RMSE 0.7063, MAE 0.4994, \n",
      "test loss: 6.5956, RMSE 0.7134, MAE 0.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.11028840392827988: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.74s/it]\n",
      "validate for the 0-th batch, validate loss: 7.128688812255859: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 7.031285762786865: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, learning rate: 0.0009757729755661011, train loss: 0.1103, RMSE 0.1713, MAE 0.0450, \n",
      "validate loss: 7.1287, RMSE 0.7137, MAE 0.5115, \n",
      "test loss: 7.0313, RMSE 0.7130, MAE 0.5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.10455526411533356: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.74s/it]\n",
      "validate for the 0-th batch, validate loss: 7.241085052490234: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n",
      "test for the 0-th batch, test loss: 7.138416290283203: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, learning rate: 0.0009733122526196349, train loss: 0.1046, RMSE 0.1644, MAE 0.0443, \n",
      "validate loss: 7.2411, RMSE 0.7069, MAE 0.4999, \n",
      "test loss: 7.1384, RMSE 0.7077, MAE 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.1393265277147293: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.73s/it]\n",
      "validate for the 0-th batch, validate loss: 7.243224143981934: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 7.140832901000977: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, learning rate: 0.0009707359806323416, train loss: 0.1393, RMSE 0.2007, MAE 0.0578, \n",
      "validate loss: 7.2432, RMSE 0.7071, MAE 0.5007, \n",
      "test loss: 7.1408, RMSE 0.7115, MAE 0.5074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.12462180852890015: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 7.027373313903809: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 6.869161605834961: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, learning rate: 0.0009680447952607845, train loss: 0.1246, RMSE 0.1881, MAE 0.0568, \n",
      "validate loss: 7.0274, RMSE 0.7132, MAE 0.5088, \n",
      "test loss: 6.8692, RMSE 0.7137, MAE 0.5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.10198055952787399: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.72s/it]\n",
      "validate for the 0-th batch, validate loss: 6.420993328094482: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 6.2541375160217285: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, learning rate: 0.0009652393605146844, train loss: 0.1020, RMSE 0.1713, MAE 0.0483, \n",
      "validate loss: 6.4210, RMSE 0.7070, MAE 0.5011, \n",
      "test loss: 6.2541, RMSE 0.7070, MAE 0.5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07757443934679031: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.73s/it]\n",
      "validate for the 0-th batch, validate loss: 5.789831638336182: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 5.731817722320557: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, learning rate: 0.000962320368593087, train loss: 0.0776, RMSE 0.1490, MAE 0.0399, \n",
      "validate loss: 5.7898, RMSE 0.7066, MAE 0.4996, \n",
      "test loss: 5.7318, RMSE 0.7194, MAE 0.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09226123243570328: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.72s/it]\n",
      "validate for the 0-th batch, validate loss: 5.24920129776001: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 5.13618278503418: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, learning rate: 0.0009592885397135706, train loss: 0.0923, RMSE 0.1609, MAE 0.0420, \n",
      "validate loss: 5.2492, RMSE 0.7119, MAE 0.5079, \n",
      "test loss: 5.1362, RMSE 0.7189, MAE 0.5175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.1260513961315155: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.64s/it]\n",
      "validate for the 0-th batch, validate loss: 4.752020835876465: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n",
      "test for the 0-th batch, test loss: 4.559947490692139: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, learning rate: 0.0009561446219345453, train loss: 0.1261, RMSE 0.1812, MAE 0.0488, \n",
      "validate loss: 4.7520, RMSE 0.7115, MAE 0.5076, \n",
      "test loss: 4.5599, RMSE 0.7061, MAE 0.5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.11438461393117905: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 4.247755527496338: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 4.112473964691162: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, learning rate: 0.0009528893909706797, train loss: 0.1144, RMSE 0.1758, MAE 0.0493, \n",
      "validate loss: 4.2478, RMSE 0.7049, MAE 0.4985, \n",
      "test loss: 4.1125, RMSE 0.7058, MAE 0.5007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09098824858665466: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.72s/it]\n",
      "validate for the 0-th batch, validate loss: 3.8328635692596436: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n",
      "test for the 0-th batch, test loss: 3.7213687896728516: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, learning rate: 0.0009495236500015047, train loss: 0.0910, RMSE 0.1484, MAE 0.0425, \n",
      "validate loss: 3.8329, RMSE 0.7094, MAE 0.5062, \n",
      "test loss: 3.7214, RMSE 0.7108, MAE 0.5082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08955284208059311: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.67s/it]\n",
      "validate for the 0-th batch, validate loss: 3.499812602996826: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n",
      "test for the 0-th batch, test loss: 3.3473198413848877: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, learning rate: 0.0009460482294732421, train loss: 0.0896, RMSE 0.1587, MAE 0.0527, \n",
      "validate loss: 3.4998, RMSE 0.7117, MAE 0.5120, \n",
      "test loss: 3.3473, RMSE 0.7032, MAE 0.4973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08545646071434021: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.73s/it]\n",
      "validate for the 0-th batch, validate loss: 3.202669858932495: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 3.0954713821411133: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, learning rate: 0.0009424639868939033, train loss: 0.0855, RMSE 0.1515, MAE 0.0532, \n",
      "validate loss: 3.2027, RMSE 0.7019, MAE 0.4981, \n",
      "test loss: 3.0955, RMSE 0.7018, MAE 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09109421819448471: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.76s/it]\n",
      "validate for the 0-th batch, validate loss: 3.0758140087127686: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n",
      "test for the 0-th batch, test loss: 2.941767692565918: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, learning rate: 0.0009387718066217125, train loss: 0.0911, RMSE 0.1588, MAE 0.0572, \n",
      "validate loss: 3.0758, RMSE 0.7114, MAE 0.5128, \n",
      "test loss: 2.9418, RMSE 0.7042, MAE 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09987720847129822: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 2.926013708114624: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 2.805070400238037: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, learning rate: 0.0009349725996469047, train loss: 0.0999, RMSE 0.1670, MAE 0.0597, \n",
      "validate loss: 2.9260, RMSE 0.7024, MAE 0.5026, \n",
      "test loss: 2.8051, RMSE 0.6996, MAE 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.1032847985625267: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.77s/it]\n",
      "validate for the 0-th batch, validate loss: 2.9740984439849854: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it]\n",
      "test for the 0-th batch, test loss: 2.8836328983306885: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, learning rate: 0.0009310673033669522, train loss: 0.1033, RMSE 0.1672, MAE 0.0730, \n",
      "validate loss: 2.9741, RMSE 0.7113, MAE 0.5177, \n",
      "test loss: 2.8836, RMSE 0.7059, MAE 0.5070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.10456344485282898: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.64s/it]\n",
      "validate for the 0-th batch, validate loss: 3.0929551124572754: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 2.96618390083313: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, learning rate: 0.0009270568813552756, train loss: 0.1046, RMSE 0.1710, MAE 0.0684, \n",
      "validate loss: 3.0930, RMSE 0.7088, MAE 0.5097, \n",
      "test loss: 2.9662, RMSE 0.7005, MAE 0.4955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09921441972255707: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 3.2568862438201904: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 3.1952099800109863: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, learning rate: 0.0009229423231234975, train loss: 0.0992, RMSE 0.1651, MAE 0.0626, \n",
      "validate loss: 3.2569, RMSE 0.7088, MAE 0.5082, \n",
      "test loss: 3.1952, RMSE 0.7158, MAE 0.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09460209310054779: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.66s/it]\n",
      "validate for the 0-th batch, validate loss: 3.4262659549713135: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "test for the 0-th batch, test loss: 3.3318870067596436: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, learning rate: 0.0009187246438772938, train loss: 0.0946, RMSE 0.1660, MAE 0.0585, \n",
      "validate loss: 3.4263, RMSE 0.7084, MAE 0.5054, \n",
      "test loss: 3.3319, RMSE 0.7029, MAE 0.4984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08882572501897812: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 3.6594724655151367: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "test for the 0-th batch, test loss: 3.610327959060669: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, learning rate: 0.0009144048842659081, train loss: 0.0888, RMSE 0.1561, MAE 0.0559, \n",
      "validate loss: 3.6595, RMSE 0.7127, MAE 0.5148, \n",
      "test loss: 3.6103, RMSE 0.7167, MAE 0.5214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09137170761823654: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.69s/it]\n",
      "validate for the 0-th batch, validate loss: 3.8259384632110596: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 3.7719385623931885: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, learning rate: 0.0009099841101253866, train loss: 0.0914, RMSE 0.1580, MAE 0.0500, \n",
      "validate loss: 3.8259, RMSE 0.7046, MAE 0.4982, \n",
      "test loss: 3.7719, RMSE 0.7100, MAE 0.5065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.1021508201956749: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 4.020791530609131: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 3.967013120651245: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, learning rate: 0.000905463412215599, train loss: 0.1022, RMSE 0.1720, MAE 0.0534, \n",
      "validate loss: 4.0208, RMSE 0.7072, MAE 0.5059, \n",
      "test loss: 3.9670, RMSE 0.7108, MAE 0.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08438368141651154: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 4.205685138702393: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 4.142058849334717: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, learning rate: 0.0009008439059511099, train loss: 0.0844, RMSE 0.1553, MAE 0.0471, \n",
      "validate loss: 4.2057, RMSE 0.7059, MAE 0.5009, \n",
      "test loss: 4.1421, RMSE 0.7057, MAE 0.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.11698751151561737: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 4.410510063171387: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 4.30321741104126: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, learning rate: 0.0008961267311259666, train loss: 0.1170, RMSE 0.1844, MAE 0.0577, \n",
      "validate loss: 4.4105, RMSE 0.7147, MAE 0.5138, \n",
      "test loss: 4.3032, RMSE 0.7078, MAE 0.5055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07340999692678452: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.66s/it]\n",
      "validate for the 0-th batch, validate loss: 4.507643699645996: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 4.404514312744141: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, learning rate: 0.0008913130516324732, train loss: 0.0734, RMSE 0.1430, MAE 0.0435, \n",
      "validate loss: 4.5076, RMSE 0.7155, MAE 0.5148, \n",
      "test loss: 4.4045, RMSE 0.7063, MAE 0.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.0938287228345871: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.69s/it]\n",
      "validate for the 0-th batch, validate loss: 4.565399169921875: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 4.4965972900390625: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, learning rate: 0.0008864040551740157, train loss: 0.0938, RMSE 0.1666, MAE 0.0486, \n",
      "validate loss: 4.5654, RMSE 0.7064, MAE 0.5004, \n",
      "test loss: 4.4966, RMSE 0.7086, MAE 0.5053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07634413987398148: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.75s/it]\n",
      "validate for the 0-th batch, validate loss: 4.6152215003967285: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 4.552709579467773: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, learning rate: 0.0008814009529720154, train loss: 0.0763, RMSE 0.1477, MAE 0.0455, \n",
      "validate loss: 4.6152, RMSE 0.7064, MAE 0.4996, \n",
      "test loss: 4.5527, RMSE 0.7067, MAE 0.5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07463399320840836: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.76s/it]\n",
      "validate for the 0-th batch, validate loss: 4.690349578857422: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 4.599159240722656: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, learning rate: 0.0008763049794670775, train loss: 0.0746, RMSE 0.1484, MAE 0.0466, \n",
      "validate loss: 4.6903, RMSE 0.7113, MAE 0.5077, \n",
      "test loss: 4.5992, RMSE 0.7066, MAE 0.4997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07832842320203781: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 4.7026190757751465: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 4.650888919830322: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, learning rate: 0.0008711173920144117, train loss: 0.0783, RMSE 0.1530, MAE 0.0498, \n",
      "validate loss: 4.7026, RMSE 0.7065, MAE 0.4996, \n",
      "test loss: 4.6509, RMSE 0.7074, MAE 0.5029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.0901101604104042: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.64s/it]\n",
      "validate for the 0-th batch, validate loss: 4.745063781738281: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n",
      "test for the 0-th batch, test loss: 4.653391361236572: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, learning rate: 0.0008658394705735987, train loss: 0.0901, RMSE 0.1645, MAE 0.0532, \n",
      "validate loss: 4.7451, RMSE 0.7160, MAE 0.5151, \n",
      "test loss: 4.6534, RMSE 0.7067, MAE 0.4997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.10014955699443817: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 4.662172794342041: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "test for the 0-th batch, test loss: 4.603079795837402: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, learning rate: 0.0008604725173927785, train loss: 0.1001, RMSE 0.1727, MAE 0.0561, \n",
      "validate loss: 4.6622, RMSE 0.7105, MAE 0.5067, \n",
      "test loss: 4.6031, RMSE 0.7066, MAE 0.4997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07446030527353287: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.65s/it]\n",
      "validate for the 0-th batch, validate loss: 4.617339134216309: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "test for the 0-th batch, test loss: 4.568633556365967: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, learning rate: 0.000855017856687341, train loss: 0.0745, RMSE 0.1487, MAE 0.0477, \n",
      "validate loss: 4.6173, RMSE 0.7111, MAE 0.5092, \n",
      "test loss: 4.5686, RMSE 0.7069, MAE 0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07066702842712402: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 4.572122097015381: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n",
      "test for the 0-th batch, test loss: 4.5286478996276855: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, learning rate: 0.0008494768343131955, train loss: 0.0707, RMSE 0.1434, MAE 0.0437, \n",
      "validate loss: 4.5721, RMSE 0.7099, MAE 0.5067, \n",
      "test loss: 4.5286, RMSE 0.7077, MAE 0.5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08559656143188477: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 4.52484130859375: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 4.463427543640137: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, learning rate: 0.0008438508174347009, train loss: 0.0856, RMSE 0.1569, MAE 0.0487, \n",
      "validate loss: 4.5248, RMSE 0.7130, MAE 0.5119, \n",
      "test loss: 4.4634, RMSE 0.7064, MAE 0.4996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07563100755214691: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.73s/it]\n",
      "validate for the 0-th batch, validate loss: 4.445233345031738: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it]\n",
      "test for the 0-th batch, test loss: 4.434951305389404: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, learning rate: 0.0008381411941873378, train loss: 0.0756, RMSE 0.1440, MAE 0.0423, \n",
      "validate loss: 4.4452, RMSE 0.7117, MAE 0.5078, \n",
      "test loss: 4.4350, RMSE 0.7126, MAE 0.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.09467941522598267: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.73s/it]\n",
      "validate for the 0-th batch, validate loss: 4.334384441375732: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "test for the 0-th batch, test loss: 4.322387218475342: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, learning rate: 0.0008323493733352077, train loss: 0.0947, RMSE 0.1684, MAE 0.0486, \n",
      "validate loss: 4.3344, RMSE 0.7059, MAE 0.4999, \n",
      "test loss: 4.3224, RMSE 0.7064, MAE 0.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07131103426218033: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "validate for the 0-th batch, validate loss: 4.277066707611084: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "test for the 0-th batch, test loss: 4.293710708618164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, learning rate: 0.000826476783923441, train loss: 0.0713, RMSE 0.1431, MAE 0.0415, \n",
      "validate loss: 4.2771, RMSE 0.7058, MAE 0.4995, \n",
      "test loss: 4.2937, RMSE 0.7122, MAE 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08477021008729935: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.75s/it]\n",
      "validate for the 0-th batch, validate loss: 4.221643447875977: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]\n",
      "test for the 0-th batch, test loss: 4.225545406341553: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, learning rate: 0.0008205248749256015, train loss: 0.0848, RMSE 0.1537, MAE 0.0465, \n",
      "validate loss: 4.2216, RMSE 0.7059, MAE 0.5006, \n",
      "test loss: 4.2255, RMSE 0.7062, MAE 0.4994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07895772904157639: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.73s/it]\n",
      "validate for the 0-th batch, validate loss: 4.220466613769531: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 4.212334632873535: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, learning rate: 0.0008144951148861742, train loss: 0.0790, RMSE 0.1481, MAE 0.0466, \n",
      "validate loss: 4.2205, RMSE 0.7105, MAE 0.5091, \n",
      "test loss: 4.2123, RMSE 0.7062, MAE 0.4994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08508030325174332: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.66s/it]\n",
      "validate for the 0-th batch, validate loss: 4.299162864685059: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n",
      "test for the 0-th batch, test loss: 4.308590888977051: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, learning rate: 0.0008083889915582234, train loss: 0.0851, RMSE 0.1536, MAE 0.0557, \n",
      "validate loss: 4.2992, RMSE 0.7111, MAE 0.5097, \n",
      "test loss: 4.3086, RMSE 0.7099, MAE 0.5079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07575006783008575: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 4.358643054962158: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s]\n",
      "test for the 0-th batch, test loss: 4.394991874694824: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, learning rate: 0.0008022080115363127, train loss: 0.0758, RMSE 0.1466, MAE 0.0483, \n",
      "validate loss: 4.3586, RMSE 0.7062, MAE 0.5000, \n",
      "test loss: 4.3950, RMSE 0.7109, MAE 0.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.0661628395318985: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.74s/it]\n",
      "validate for the 0-th batch, validate loss: 4.430382251739502: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 4.464009761810303: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59, learning rate: 0.0007959536998847743, train loss: 0.0662, RMSE 0.1390, MAE 0.0445, \n",
      "validate loss: 4.4304, RMSE 0.7071, MAE 0.5036, \n",
      "test loss: 4.4640, RMSE 0.7066, MAE 0.4997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.08290460705757141: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it]\n",
      "validate for the 0-th batch, validate loss: 4.509982109069824: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s]\n",
      "test for the 0-th batch, test loss: 4.528283596038818: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, learning rate: 0.0007896275997614229, train loss: 0.0829, RMSE 0.1560, MAE 0.0533, \n",
      "validate loss: 4.5100, RMSE 0.7068, MAE 0.5020, \n",
      "test loss: 4.5283, RMSE 0.7067, MAE 0.4997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for the 0-th batch, train loss: 0.07855940610170364: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it]\n",
      "validate for the 0-th batch, validate loss: 4.581945419311523: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "test for the 0-th batch, test loss: 4.646113395690918: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, learning rate: 0.0007832312720368048, train loss: 0.0786, RMSE 0.1499, MAE 0.0503, \n",
      "validate loss: 4.5819, RMSE 0.7064, MAE 0.4996, \n",
      "test loss: 4.6461, RMSE 0.7123, MAE 0.5082\n",
      "save as ../results/lfm1b_listened_to_artist/rhgnn.json\n",
      "predicted relation: listened_to_artist\n",
      "result: {\n",
      "    \"RMSE\": 0.5956,\n",
      "    \"MAE\": 0.426\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "train_steps = 0\n",
    "best_validate_RMSE, final_result = None, None\n",
    "loss_values=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_y_trues = []\n",
    "    train_y_predicts = []\n",
    "    train_total_loss = 0.0\n",
    "    train_loader_tqdm = tqdm(train_loader, ncols=120)\n",
    "\n",
    "    for batch, (input_nodes, positive_graph, negative_graph, blocks) in enumerate(train_loader_tqdm):\n",
    "        blocks = [convert_to_gpu(b, device=DEVICE) for b in blocks]\n",
    "        positive_graph, negative_graph = convert_to_gpu(\n",
    "            positive_graph, \n",
    "            negative_graph, device=DEVICE)\n",
    "\n",
    "        # target node relation representation in the heterogeneous graph\n",
    "        input_features = {(stype, etype, dtype): blocks[0].srcnodes[dtype].data['feat'] for stype, etype, dtype in blocks[0].canonical_etypes}\n",
    "        # for k,v in input_features.items():\n",
    "        #     print(k,v.shape)\n",
    "        nodes_representation, _ = model[0](blocks, copy.deepcopy(input_features))\n",
    "        # for k,v in nodes_representation.items():\n",
    "        #     print(k,v.shape)\n",
    "        \n",
    "        positive_score = model[1](\n",
    "            positive_graph, \n",
    "            nodes_representation, \n",
    "            SAMPLED_EDGE_TYPE).squeeze(dim=-1)\n",
    "        negative_score = model[1](\n",
    "            negative_graph, \n",
    "            nodes_representation, \n",
    "            SAMPLED_EDGE_TYPE).squeeze(dim=-1)\n",
    "\n",
    "\n",
    "        train_y_predict = th.cat([positive_score, negative_score], dim=0)\n",
    "        train_y_true = th.cat(\n",
    "            [th.ones_like(positive_score), \n",
    "            th.zeros_like(negative_score)], dim=0)\n",
    "        loss = loss_func(train_y_predict, train_y_true)\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "        loss_values.append(loss.item())\n",
    "        train_y_trues.append(train_y_true.detach().cpu())\n",
    "        train_y_predicts.append(train_y_predict.detach().cpu())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loader_tqdm.set_description(f'training for the {batch}-th batch, train loss: {loss.item()}')\n",
    "\n",
    "        # step should be called after a batch has been used for training.\n",
    "        train_steps += 1\n",
    "        scheduler.step(train_steps)\n",
    "\n",
    "    train_total_loss /= (batch + 1)\n",
    "    train_y_trues = th.cat(train_y_trues, dim=0)\n",
    "    train_y_predicts = th.cat(train_y_predicts, dim=0)\n",
    "\n",
    "    train_RMSE, train_MAE = evaluate_link_prediction(\n",
    "        predict_scores=train_y_predicts, \n",
    "        true_scores=train_y_trues)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_total_loss, val_y_trues, val_y_predicts = evaluate(\n",
    "        model, \n",
    "        loader=val_loader, \n",
    "        loss_func=loss_func,\n",
    "        sampled_edge_type=SAMPLED_EDGE_TYPE,\n",
    "        device=DEVICE, \n",
    "        mode='validate')\n",
    "\n",
    "    val_RMSE, val_MAE = evaluate_link_prediction(\n",
    "        predict_scores=val_y_predicts,\n",
    "        true_scores=val_y_trues)\n",
    "\n",
    "    test_total_loss, test_y_trues, test_y_predicts = evaluate(\n",
    "        model, \n",
    "        loader=test_loader, \n",
    "        loss_func=loss_func,\n",
    "        sampled_edge_type=SAMPLED_EDGE_TYPE,\n",
    "        device=DEVICE, \n",
    "        mode='test')\n",
    "\n",
    "    test_RMSE, test_MAE = evaluate_link_prediction(\n",
    "        predict_scores=test_y_predicts,\n",
    "        true_scores=test_y_trues)\n",
    "\n",
    "    if best_validate_RMSE is None or val_RMSE < best_validate_RMSE:\n",
    "        best_validate_RMSE = val_RMSE\n",
    "        scores = {\"RMSE\": float(f\"{test_RMSE:.4f}\"), \"MAE\": float(f\"{test_MAE:.4f}\")}\n",
    "        final_result = json.dumps(scores, indent=4)\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch}, learning rate: {optimizer.param_groups[0][\"lr\"]}, train loss: {train_total_loss:.4f}, RMSE {train_RMSE:.4f}, MAE {train_MAE:.4f}, \\n'\n",
    "        f'validate loss: {val_total_loss:.4f}, RMSE {val_RMSE:.4f}, MAE {val_MAE:.4f}, \\n'\n",
    "        f'test loss: {test_total_loss:.4f}, RMSE {test_RMSE:.4f}, MAE {test_MAE:.4f}')\n",
    "\n",
    "    early_stop = early_stopping.step([('RMSE', val_RMSE, False), ('MAE', val_MAE, False)], model)\n",
    "\n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "\n",
    "# save the model result\n",
    "\n",
    "SAVE_RESULT_FOLDER= f\"../results/lfm1b_{SAMPLED_EDGE_TYPE}\"\n",
    "if not os.path.exists(SAVE_RESULT_FOLDER):\n",
    "    os.makedirs(SAVE_RESULT_FOLDER, exist_ok=True)\n",
    "save_result_path = os.path.join(SAVE_RESULT_FOLDER, f\"rhgnn.json\")\n",
    "\n",
    "with open(save_result_path, 'w') as file:\n",
    "    file.write(final_result)\n",
    "    file.close()\n",
    "\n",
    "print(f'save as {save_result_path}')\n",
    "print(f\"predicted relation: {SAMPLED_EDGE_TYPE}\")\n",
    "print(f'result: {final_result}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3d04349b985196cea633a6a40d4aa215901f3ae3a067a6dfdb310b02f168d27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dev-CLxekTzI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
