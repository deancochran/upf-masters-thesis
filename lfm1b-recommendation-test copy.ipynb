{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 21:03:19.603063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-31 21:03:19.603088: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch as th \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from DGL_LFM1b.data_utils import *\n",
    "from utils.utils import convert_to_gpu\n",
    "from dgl.data.utils import load_graphs\n",
    "from utils.LinkScorePredictor import LinkScorePredictor\n",
    "from model.R_HGNN import R_HGNN\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: right;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Arugments and initializing graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation with Link Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DGL_LFM1b.data_utils import get_fileSize, get_col_names, setType, isValid, get_preprocessed_ids\n",
    "\n",
    "def get_ids_from_txtFile(path_txt_file, type ,ids):\n",
    "    chunksize=1000000\n",
    "    chunks=[]\n",
    "    size = get_fileSize(path_txt_file)\n",
    "    col_names=get_col_names(type)\n",
    "    df_chunks = pd.read_csv(path_txt_file, names=col_names, sep=\"\\t\", encoding='utf8', header = 0, chunksize=chunksize)\n",
    "    for chunk in tqdm(df_chunks, total=size//chunksize):\n",
    "        for col in chunk.columns:\n",
    "            try:\n",
    "                chunk[col]=chunk[col].apply(lambda x: setType(x, col))\n",
    "            except:\n",
    "                chunk[col]=chunk[col].apply(lambda x: isValid(x, col))\n",
    "                chunk[col]=chunk[col].apply(lambda x: setType(x, col))\n",
    "        chunk = chunk[chunk[col_names[0]].isin(ids)] \n",
    "        chunks.append(chunk)\n",
    "    df = pd.concat(chunks)\n",
    "    # print(f'{type} df has shpe:',df.shape)\n",
    "    return df\n",
    "\n",
    "def topK_playcounts(path_to_txt_data, series, type, k, user_id):\n",
    "    val_count_pairs=series.value_counts().head(k)\n",
    "    ids=list(val_count_pairs.keys())\n",
    "    type_df = get_ids_from_txtFile(path_to_txt_data, type, ids)\n",
    "    print(f'Top {k} {type}s for user {user_id}')\n",
    "    print('ID       PLAYCOUNT      NAME')\n",
    "    for id, playcount in val_count_pairs.items():\n",
    "        row=type_df.loc[type_df[type+'_id'] == id]\n",
    "        name = row[type+'_name'].item()\n",
    "        print(f'{id}        {playcount}         {name} ')\n",
    "\n",
    "def topK_recommendations(path_to_txt_data, ids, type, k, user_id, artist_path_to_txt_data=None):\n",
    "    type_df = get_ids_from_txtFile(path_to_txt_data, type, ids)\n",
    "    if artist_path_to_txt_data != None:\n",
    "        artist_ids=[type_df.loc[type_df[type+'_id'] == id]['artist_id'].item() for id in ids]\n",
    "        artist_df= get_ids_from_txtFile(artist_path_to_txt_data, type='artist', ids=artist_ids)\n",
    "        artist_name_mapping={id: artist_df.loc[artist_df['artist_id'] == id]['artist_name'].item() for id in artist_ids}\n",
    "\n",
    "    print(f'Top {k} {type} recommendations for user #{user_id}')\n",
    "    if artist_path_to_txt_data != None:\n",
    "        print('ID           ARTIST_NAME             NAME')\n",
    "    else:\n",
    "        print('ID             NAME')\n",
    "    for id in ids:\n",
    "        row=type_df.loc[type_df[type+'_id'] == id]\n",
    "        name = row[type+'_name'].item()\n",
    "        try:\n",
    "            artist_id = row['artist_id'].item()\n",
    "            print(f'{id}        {artist_name_mapping[artist_id]}        {name}')\n",
    "        except:\n",
    "            print(f'{id}  {name}')\n",
    "\n",
    "def displayTopKRecommendations(graph, model, type, sampled_edge_type, k, user_id, typeFile_path, userFile_path, artists_path, albums_path, tracks_path, les_path, device, artist_path_to_txt_data=None):\n",
    "    input_features = {(stype, etype, dtype): graph.srcnodes[dtype].data['feat'] for stype, etype, dtype in graph.canonical_etypes}\n",
    "    nodes_representation, _ = model[0].inference(graph, copy.deepcopy(input_features), device=device)\n",
    "    user_nodes_representation=nodes_representation['user']\n",
    "    type_nodes_representation=nodes_representation[type]\n",
    "    # C = torch.mm(A, B.T)  # same as C = A @ B.T\n",
    "    listen_to_type_likelihood = th.mm(user_nodes_representation, type_nodes_representation.T)\n",
    "\n",
    "    print(f'listen_to_{type}_likelihood',listen_to_type_likelihood.shape)\n",
    "    user_type_recommendations={}\n",
    "    for u_id, row in enumerate(tqdm(listen_to_type_likelihood, total=int(listen_to_type_likelihood.shape[0]))):\n",
    "        for id, _ in enumerate(row):\n",
    "            try:\n",
    "                graph.edge_id(u_id,id, etype=sampled_edge_type)\n",
    "            except:\n",
    "                if u_id in user_type_recommendations.keys():\n",
    "                    user_type_recommendations[u_id].append((u_id, id, listen_to_type_likelihood[u_id,id].item()))\n",
    "                else:\n",
    "                    user_type_recommendations[u_id]=list()\n",
    "                    user_type_recommendations[u_id].append((u_id, id, listen_to_type_likelihood[u_id,id].item()))\n",
    "\n",
    "    rev_user_mapping = get_id_mapping(path_to_file=userFile_path, type='user', reverse=True)\n",
    "    rev_type_mapping = get_id_mapping(path_to_file=typeFile_path, type=type, reverse=True)\n",
    "    user_listens=get_ids_from_txtFile(path_txt_file=les_path, type='le' , ids=[rev_user_mapping[user_id]])\n",
    "    topK_playcounts(path_to_txt_data=artists_path, series=user_listens['artist_id'], type='artist', k=k, user_id=user_id)\n",
    "    topK_playcounts(path_to_txt_data=albums_path, series=user_listens['album_id'], type='album', k=k, user_id=user_id)\n",
    "    topK_playcounts(path_to_txt_data=tracks_path, series=user_listens['track_id'], type='track', k=k, user_id=user_id)\n",
    "    user_type_recommendations={key: sorted(value, key=lambda x: x[2], reverse=True)[:k] for key, value in  user_type_recommendations.items()}\n",
    "    user_type_recommendations=[rev_type_mapping[id] for _, id, _ in user_type_recommendations[user_id]]\n",
    "    if artist_path_to_txt_data !=None:\n",
    "        topK_recommendations(path_to_txt_data=typeFile_path, ids=user_type_recommendations, type=type, k=k, user_id=user_id, artist_path_to_txt_data=artists_path)\n",
    "    else:\n",
    "        topK_recommendations(path_to_txt_data=typeFile_path, ids=user_type_recommendations, type=type, k=k, user_id=user_id)\n",
    "\n",
    "def get_result_folder_path(root, date, sample_edge_type):\n",
    "    return f'{root}/lfm1b/{date}/{sample_edge_type}'\n",
    "\n",
    "def get_result_folder_args(root, date, sample_edge_type):\n",
    "    return get_result_folder_path(root, date, sample_edge_type)+'/args.json'\n",
    "\n",
    "def get_result_folder_model_state(root, date, sample_edge_type):\n",
    "    return get_result_folder_path(root, date, sample_edge_type)+f'/{sample_edge_type}.pkl'\n",
    "\n",
    "def get_result_folder_metrics(root, date, sample_edge_type):\n",
    "    return get_result_folder_path(root, date, sample_edge_type)+f'/metrics.pkl'\n",
    "\n",
    "\n",
    "def get_id_mapping(path_to_file, type, reverse=False):\n",
    "    ids=get_preprocessed_ids(path_to_file, return_unique_ids=False, type=type, id_list=get_col_names(type))[f'{type}_id']\n",
    "    if reverse:\n",
    "        return {i: row for i, row in enumerate(ids)}\n",
    "    else:\n",
    "        return {row: i for i, row in enumerate(ids)}\n",
    "\n",
    "\n",
    "def build_model(data_post_path,  date, args, sample_edge_type, root='results/'):\n",
    "    model_state_path=get_result_folder_model_state(root, date, sample_edge_type)\n",
    "    glist,_=load_graphs(f'{data_post_path}/lastfm1b.bin')\n",
    "    hg=glist[0]\n",
    "    r_hgnn = R_HGNN(graph=hg,\n",
    "                input_dim_dict={ntype: 8 for ntype in hg.ntypes},\n",
    "                hidden_dim=16, \n",
    "                relation_input_dim=8,\n",
    "                relation_hidden_dim=16,\n",
    "                num_layers=2, \n",
    "                n_heads=args['num_heads'], \n",
    "                dropout=args['dropout'],\n",
    "                residual=args['residual'], \n",
    "                norm=args['norm'])\n",
    "    link_scorer = LinkScorePredictor(16 * args['num_heads'])\n",
    "\n",
    "    model = nn.Sequential(r_hgnn, link_scorer)\n",
    "    model = convert_to_gpu(model, device=args['device'])\n",
    "    model.load_state_dict(th.load(model_state_path))\n",
    "    return model\n",
    "\n",
    "def get_file_pre_path(data_pre_path, type):\n",
    "    print('type:',type)\n",
    "    if type=='user':\n",
    "        return data_pre_path+'LFM-1b_users.txt'\n",
    "    elif type=='album':\n",
    "        return data_pre_path+'LFM-1b_albums.txt'\n",
    "    elif type=='artist':\n",
    "        return data_pre_path+'LFM-1b_artists.txt'\n",
    "    elif type=='track':\n",
    "        return data_pre_path+'LFM-1b_tracks.txt'\n",
    "    elif type=='le':\n",
    "        return data_pre_path+'LFM-1b_LEs.txt'\n",
    "    elif type=='genre':\n",
    "        return data_pre_path+'genres_allmusic.txt'\n",
    "    else:\n",
    "        raise Exception('bad \"type\" parameter in get_col_names')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing LFM1b\n"
     ]
    }
   ],
   "source": [
    "from DGL_LFM1b.DGL_LFM1b import LFM1b\n",
    "dataset=LFM1b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_recommendations(graph, data_pre_path, data_post_path,  date, sample_edge_type, root, u_id, k):\n",
    "    args_path=get_result_folder_args(root, date, sample_edge_type)\n",
    "    args = json.load(open(args_path))\n",
    "    print('args',args)\n",
    "    model=build_model(data_post_path,  date, args, sample_edge_type, root='./results/')\n",
    "\n",
    "    displayTopKRecommendations(\n",
    "    graph=graph,\n",
    "    model=model,\n",
    "    type='artist',\n",
    "    sampled_edge_type=sample_edge_type,\n",
    "    k=k,\n",
    "    user_id=u_id,\n",
    "    typeFile_path=get_file_pre_path(data_pre_path,type='artist'),\n",
    "    userFile_path=data_pre_path+'LFM-1b_users.txt',\n",
    "    artists_path=data_pre_path+'LFM-1b_artists.txt',\n",
    "    albums_path=data_pre_path+'LFM-1b_albums.txt',\n",
    "    tracks_path=data_pre_path+'LFM-1b_tracks.txt',\n",
    "    les_path=data_pre_path+'LFM-1b_LEs.txt',\n",
    "    device=args['device']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args {'seed': 0, 'sample_edge_rate': 0.05, 'num_layers': 2, 'batch_size': 512, 'num_neg_samples': 10, 'node_min_neighbors': 10, 'shuffle': True, 'drop_last': False, 'num_workers': 4, 'hidden_dim': 16, 'rel_input_dim': 8, 'rel_hidden_dim': 16, 'num_heads': 8, 'dropout': 0.5, 'residual': True, 'norm': True, 'opt': 'adam', 'weight_decay': 0.0, 'epochs': 100, 'patience': 25, 'split_by_users': True, 'device': 'cuda', 'artists': True, 'albums': True, 'tracks': True, 'playcount_weight': False, 'norm_playcount_weight': False, 'metapath2vec': True, 'emb_dim': 8, 'walk_length': 64, 'context_size': 7, 'walks_per_node': 1, 'metapath2vec_epochs_batch_size': 512, 'learning_rate': 0.001, 'metapath2vec_epochs': 5, 'logs': 100, 'n_users': 25, 'popular_artists': True, 'model_parameters': 1035882}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tUnexpected key(s) in state_dict: \"0.relation_embedding.album_listened_by\", \"0.relation_embedding.listened_to_album\", \"0.relation_embedding.listened_to_track\", \"0.relation_embedding.preformed\", \"0.relation_embedding.preformed_by\", \"0.relation_embedding.produced\", \"0.relation_embedding.produced_by\", \"0.relation_embedding.track_listened_by\", \"0.projection_layer.album.weight\", \"0.projection_layer.album.bias\", \"0.projection_layer.track.weight\", \"0.projection_layer.track.bias\", \"0.layers.0.node_transformation_weight.album\", \"0.layers.0.node_transformation_weight.track\", \"0.layers.0.relation_transformation_weight.album_listened_by\", \"0.layers.0.relation_transformation_weight.listened_to_album\", \"0.layers.0.relation_transformation_weight.listened_to_track\", \"0.layers.0.relation_transformation_weight.preformed\", \"0.layers.0.relation_transformation_weight.preformed_by\", \"0.layers.0.relation_transformation_weight.produced\", \"0.layers.0.relation_transformation_weight.produced_by\", \"0.layers.0.relation_transformation_weight.track_listened_by\", \"0.layers.0.relation_propagation_layer.album_listened_by.weight\", \"0.layers.0.relation_propagation_layer.album_listened_by.bias\", \"0.layers.0.relation_propagation_layer.produced_by.weight\", \"0.layers.0.relation_propagation_layer.produced_by.bias\", \"0.layers.0.relation_propagation_layer.preformed.weight\", \"0.layers.0.relation_propagation_layer.preformed.bias\", \"0.layers.0.relation_propagation_layer.produced.weight\", \"0.layers.0.relation_propagation_layer.produced.bias\", \"0.layers.0.relation_propagation_layer.preformed_by.weight\", \"0.layers.0.relation_propagation_layer.preformed_by.bias\", \"0.layers.0.relation_propagation_layer.track_listened_by.weight\", \"0.layers.0.relation_propagation_layer.track_listened_by.bias\", \"0.layers.0.relation_propagation_layer.listened_to_album.weight\", \"0.layers.0.relation_propagation_layer.listened_to_album.bias\", \"0.layers.0.relation_propagation_layer.listened_to_track.weight\", \"0.layers.0.relation_propagation_layer.listened_to_track.bias\", \"0.layers.0.res_fc.album.weight\", \"0.layers.0.res_fc.album.bias\", \"0.layers.0.res_fc.track.weight\", \"0.layers.0.res_fc.track.bias\", \"0.layers.0.residual_weight.album\", \"0.layers.0.residual_weight.track\", \"0.layers.0.layer_norm.album.weight\", \"0.layers.0.layer_norm.album.bias\", \"0.layers.0.layer_norm.track.weight\", \"0.layers.0.layer_norm.track.bias\", \"0.layers.0.relations_crossing_attention_weight.album_listened_by\", \"0.layers.0.relations_crossing_attention_weight.listened_to_album\", \"0.layers.0.relations_crossing_attention_weight.listened_to_track\", \"0.layers.0.relations_crossing_attention_weight.preformed\", \"0.layers.0.relations_crossing_attention_weight.preformed_by\", \"0.layers.0.relations_crossing_attention_weight.produced\", \"0.layers.0.relations_crossing_attention_weight.produced_by\", \"0.layers.0.relations_crossing_attention_weight.track_listened_by\", \"0.layers.1.node_transformation_weight.album\", \"0.layers.1.node_transformation_weight.track\", \"0.layers.1.relation_transformation_weight.album_listened_by\", \"0.layers.1.relation_transformation_weight.listened_to_album\", \"0.layers.1.relation_transformation_weight.listened_to_track\", \"0.layers.1.relation_transformation_weight.preformed\", \"0.layers.1.relation_transformation_weight.preformed_by\", \"0.layers.1.relation_transformation_weight.produced\", \"0.layers.1.relation_transformation_weight.produced_by\", \"0.layers.1.relation_transformation_weight.track_listened_by\", \"0.layers.1.relation_propagation_layer.album_listened_by.weight\", \"0.layers.1.relation_propagation_layer.album_listened_by.bias\", \"0.layers.1.relation_propagation_layer.produced_by.weight\", \"0.layers.1.relation_propagation_layer.produced_by.bias\", \"0.layers.1.relation_propagation_layer.preformed.weight\", \"0.layers.1.relation_propagation_layer.preformed.bias\", \"0.layers.1.relation_propagation_layer.produced.weight\", \"0.layers.1.relation_propagation_layer.produced.bias\", \"0.layers.1.relation_propagation_layer.preformed_by.weight\", \"0.layers.1.relation_propagation_layer.preformed_by.bias\", \"0.layers.1.relation_propagation_layer.track_listened_by.weight\", \"0.layers.1.relation_propagation_layer.track_listened_by.bias\", \"0.layers.1.relation_propagation_layer.listened_to_album.weight\", \"0.layers.1.relation_propagation_layer.listened_to_album.bias\", \"0.layers.1.relation_propagation_layer.listened_to_track.weight\", \"0.layers.1.relation_propagation_layer.listened_to_track.bias\", \"0.layers.1.res_fc.album.weight\", \"0.layers.1.res_fc.album.bias\", \"0.layers.1.res_fc.track.weight\", \"0.layers.1.res_fc.track.bias\", \"0.layers.1.residual_weight.album\", \"0.layers.1.residual_weight.track\", \"0.layers.1.layer_norm.album.weight\", \"0.layers.1.layer_norm.album.bias\", \"0.layers.1.layer_norm.track.weight\", \"0.layers.1.layer_norm.track.bias\", \"0.layers.1.relations_crossing_attention_weight.album_listened_by\", \"0.layers.1.relations_crossing_attention_weight.listened_to_album\", \"0.layers.1.relations_crossing_attention_weight.listened_to_track\", \"0.layers.1.relations_crossing_attention_weight.preformed\", \"0.layers.1.relations_crossing_attention_weight.preformed_by\", \"0.layers.1.relations_crossing_attention_weight.produced\", \"0.layers.1.relations_crossing_attention_weight.produced_by\", \"0.layers.1.relations_crossing_attention_weight.track_listened_by\", \"0.node_transformation_weight.album_listened_by\", \"0.node_transformation_weight.listened_to_album\", \"0.node_transformation_weight.listened_to_track\", \"0.node_transformation_weight.preformed\", \"0.node_transformation_weight.preformed_by\", \"0.node_transformation_weight.produced\", \"0.node_transformation_weight.produced_by\", \"0.node_transformation_weight.track_listened_by\", \"0.relation_transformation_weight.album_listened_by\", \"0.relation_transformation_weight.listened_to_album\", \"0.relation_transformation_weight.listened_to_track\", \"0.relation_transformation_weight.preformed\", \"0.relation_transformation_weight.preformed_by\", \"0.relation_transformation_weight.produced\", \"0.relation_transformation_weight.produced_by\", \"0.relation_transformation_weight.track_listened_by\". \n\tsize mismatch for 0.projection_layer.artist.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([128, 32]).\n\tsize mismatch for 0.projection_layer.genre.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([128, 32]).\n\tsize mismatch for 0.projection_layer.user.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([128, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3021319/3359059903.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msample_edge_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'listened_to_artist'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpreform_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_pre_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_post_path\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_edge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3021319/4183078783.py\u001b[0m in \u001b[0;36mpreform_recommendations\u001b[0;34m(graph, data_pre_path, data_post_path, date, sample_edge_type, root, u_id, k)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_post_path\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_edge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     displayTopKRecommendations(\n",
      "\u001b[0;32m/tmp/ipykernel_3021319/2588651283.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(data_post_path, date, args, sample_edge_type, root)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_hgnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_scorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/upf-masters-thesis/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1498\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tUnexpected key(s) in state_dict: \"0.relation_embedding.album_listened_by\", \"0.relation_embedding.listened_to_album\", \"0.relation_embedding.listened_to_track\", \"0.relation_embedding.preformed\", \"0.relation_embedding.preformed_by\", \"0.relation_embedding.produced\", \"0.relation_embedding.produced_by\", \"0.relation_embedding.track_listened_by\", \"0.projection_layer.album.weight\", \"0.projection_layer.album.bias\", \"0.projection_layer.track.weight\", \"0.projection_layer.track.bias\", \"0.layers.0.node_transformation_weight.album\", \"0.layers.0.node_transformation_weight.track\", \"0.layers.0.relation_transformation_weight.album_listened_by\", \"0.layers.0.relation_transformation_weight.listened_to_album\", \"0.layers.0.relation_transformation_weight.listened_to_track\", \"0.layers.0.relation_transformation_weight.preformed\", \"0.layers.0.relation_transformation_weight.preformed_by\", \"0.layers.0.relation_transformation_weight.produced\", \"0.layers.0.relation_transformation_weight.produced_by\", \"0.layers.0.relation_transformation_weight.track_listened_by\", \"0.layers.0.relation_propagation_layer.album_listened_by.weight\", \"0.layers.0.relation_propagation_layer.album_listened_by.bias\", \"0.layers.0.relation_propagation_layer.produced_by.weight\", \"0.layers.0.relation_propagation_layer.produced_by.bias\", \"0.layers.0.relation_propagation_layer.preformed.weight\", \"0.layers.0.relation_propagation_layer.preformed.bias\", \"0.layers.0.relation_propagation_layer.produced.weight\", \"0.layers.0.relation_propagation_layer.produced.bias\", \"0.layers.0.relation_propagation_layer.preformed_by.weight\", \"0.layers.0.relation_propagation_layer.preformed_by.bias\", \"0.layers.0.relation_propagation_layer.track_listened_by.weight\", \"0.layers.0.relation_propagation_layer.track_listened_by.bias\", \"0.layers.0.relation_propagation_layer.listened_to_album.weight\", \"0.layers.0.relation_propagation_layer.listened_to_album.bias\", \"0.layers.0.relation_propagation_layer.listened_to_track.weight\", \"0.layers.0.relation_propagation_layer.listened_to_track.bias\", \"0.layers.0.res_fc.album.weight\", \"0.layers.0.res_fc.album.bias\", \"0.layers.0.res_fc.track.weight\", \"0.layers.0.res_fc.track.bias\", \"0.layers.0.residual_weight.album\", \"0.layers.0.residual_weight.track\", \"0.layers.0.layer_norm.album.weight\", \"0.layers.0.layer_norm.album.bias\", \"0.layers.0.layer_norm.track.weight\", \"0.layers.0.layer_norm.track.bias\", \"0.layers.0.relations_crossing_attention_weight.album_listened_by\", \"0.layers.0.relations_crossing_attention_weight.listened_to_album\", \"0.layers.0.relations_crossing_attention_weight.listened_to_track\", \"0.layers.0.relations_crossing_attention_weight.preformed\", \"0.layers.0.relations_crossing_attention_weight.preformed_by\", \"0.layers.0.relations_crossing_attention_weight.produced\", \"0.layers.0.relations_crossing_attention_weight.produced_by\", \"0.layers.0.relations_crossing_attention_weight.track_listened_by\", \"0.layers.1.node_transformation_weight.album\", \"0.layers.1.node_transformation_weight.track\", \"0.layers.1.relation_transformation_weight.album_listened_by\", \"0.layers.1.relation_transformation_weight.listened_to_album\", \"0.layers.1.relation_transformation_weight.listened_to_track\", \"0.layers.1.relation_transformation_weight.preformed\", \"0.layers.1.relation_transformation_weight.preformed_by\", \"0.layers.1.relation_transformation_weight.produced\", \"0.layers.1.relation_transformation_weight.produced_by\", \"0.layers.1.relation_transformation_weight.track_listened_by\", \"0.layers.1.relation_propagation_layer.album_listened_by.weight\", \"0.layers.1.relation_propagation_layer.album_listened_by.bias\", \"0.layers.1.relation_propagation_layer.produced_by.weight\", \"0.layers.1.relation_propagation_layer.produced_by.bias\", \"0.layers.1.relation_propagation_layer.preformed.weight\", \"0.layers.1.relation_propagation_layer.preformed.bias\", \"0.layers.1.relation_propagation_layer.produced.weight\", \"0.layers.1.relation_propagation_layer.produced.bias\", \"0.layers.1.relation_propagation_layer.preformed_by.weight\", \"0.layers.1.relation_propagation_layer.preformed_by.bias\", \"0.layers.1.relation_propagation_layer.track_listened_by.weight\", \"0.layers.1.relation_propagation_layer.track_listened_by.bias\", \"0.layers.1.relation_propagation_layer.listened_to_album.weight\", \"0.layers.1.relation_propagation_layer.listened_to_album.bias\", \"0.layers.1.relation_propagation_layer.listened_to_track.weight\", \"0.layers.1.relation_propagation_layer.listened_to_track.bias\", \"0.layers.1.res_fc.album.weight\", \"0.layers.1.res_fc.album.bias\", \"0.layers.1.res_fc.track.weight\", \"0.layers.1.res_fc.track.bias\", \"0.layers.1.residual_weight.album\", \"0.layers.1.residual_weight.track\", \"0.layers.1.layer_norm.album.weight\", \"0.layers.1.layer_norm.album.bias\", \"0.layers.1.layer_norm.track.weight\", \"0.layers.1.layer_norm.track.bias\", \"0.layers.1.relations_crossing_attention_weight.album_listened_by\", \"0.layers.1.relations_crossing_attention_weight.listened_to_album\", \"0.layers.1.relations_crossing_attention_weight.listened_to_track\", \"0.layers.1.relations_crossing_attention_weight.preformed\", \"0.layers.1.relations_crossing_attention_weight.preformed_by\", \"0.layers.1.relations_crossing_attention_weight.produced\", \"0.layers.1.relations_crossing_attention_weight.produced_by\", \"0.layers.1.relations_crossing_attention_weight.track_listened_by\", \"0.node_transformation_weight.album_listened_by\", \"0.node_transformation_weight.listened_to_album\", \"0.node_transformation_weight.listened_to_track\", \"0.node_transformation_weight.preformed\", \"0.node_transformation_weight.preformed_by\", \"0.node_transformation_weight.produced\", \"0.node_transformation_weight.produced_by\", \"0.node_transformation_weight.track_listened_by\", \"0.relation_transformation_weight.album_listened_by\", \"0.relation_transformation_weight.listened_to_album\", \"0.relation_transformation_weight.listened_to_track\", \"0.relation_transformation_weight.preformed\", \"0.relation_transformation_weight.preformed_by\", \"0.relation_transformation_weight.produced\", \"0.relation_transformation_weight.produced_by\", \"0.relation_transformation_weight.track_listened_by\". \n\tsize mismatch for 0.projection_layer.artist.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([128, 32]).\n\tsize mismatch for 0.projection_layer.genre.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([128, 32]).\n\tsize mismatch for 0.projection_layer.user.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([128, 32])."
     ]
    }
   ],
   "source": [
    "uer_id=0\n",
    "k=10\n",
    "graph=dataset[0]\n",
    "data_root_path='results'\n",
    "data_pre_path='data/DGL_LFM1b/preprocessed/'\n",
    "data_post_path='data/DGL_LFM1b/processed/'\n",
    "date='31_07_2022_13:10:59'\n",
    "sample_edge_type='listened_to_artist'\n",
    "\n",
    "preform_recommendations(graph, data_pre_path, data_post_path,  date, sample_edge_type, data_root_path, uer_id, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
