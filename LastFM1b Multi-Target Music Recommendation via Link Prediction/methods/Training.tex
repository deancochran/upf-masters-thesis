\section{Training Graph Neural Networks}
Due to the intricacies of graph neural networks, there are some noteworthy training differences that need to be used specifically for large graph representations like the DGL LastFM1b heterogeneous graph. These differences in training will be briefly outlined in the following sections, with additional task specific information to be outlined later.

\subsection{Training for large graphs}
As it can be observed from many of the possible scenarios that heterogeneous graph networks can be applied to. Common limitations of these networks reside in the number of computations required to compute predictions. The LastFM1b graph for instance has millions of nodes and billions of edges. \cite{Schedl2016}

This can severely limit the ability to train a graph network model as a naive full batch machine learning approach must load the full graph in memory, compute new embedding representations for every node, for each GNN layer, compute the loss, and preform gradient descent to optimize the scoring function. This is highly inefficient to do, and cause limitations when attempting to utilize GPUs of limited storage capacity. As a solution mini-batch training is often utilized to efficiently increase the computation speed of large graph networks.

\subsection{Mini batch training}For the specified training spilt of the graph, a series of computations for each node needs to be meet. For each target node, the neighbors of the node are used in the message passing aggregation and update function. The objective is to fit the necessary computations for all the target nodes into a particular batch of the training data. However, this may not always be feasible to fit each target node's small graphs in a huge computation graph due to the high connectivity of a certain target node's neighbors. For this reason, GNNs employ a sampling function, such that a subset of the target node's neighbors is used. This allows the computation graph to be bounded in size such that they can be utilized for large network training.

\subsubsection{Neighbor Sampling}
As discussed above in, graph networks generate a single node embedding representations utilizing aggregation functions. Within the context of a neural network layer, the aggregation search depth utilized to update of a target node's embedding representations increases with each layer in the network. As an example, a GNN with two layers generates embed dings for its nodes using 2-hop neighborhood structure. As a generalization of this example, you can say that a k-layer GNN generates an embedding for its nodes using K-hop neighborhood structure.


Notice that for each target node that is updated, only K-hop neighbors are required to interpret the updated representation. This is a core understanding to interpret the reason for why GNNs use neighborhood sampling for faster computational speed and less memory allocation.